{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.transforms import ToTensor\n",
    "import torchvision.utils as vutils\n",
    "import matplotlib.animation as animation\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing the MNIST dataset\n",
    "\n",
    "To normalize input data properly, we need to find the `mean` & `std` of the whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.MNIST(root='MNIST', download=True,transform=transforms.Compose([\n",
    "    transforms.ToTensor(),  # Convert the image to `numpy.ndarray`\n",
    "]))\n",
    "\n",
    "mnist_mean, mnist_std = dataset.train_data.float().mean()/255, dataset.train_data.float().std()/255\n",
    "print(f\"MNIST mean={mnist_mean} & std={mnist_std}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading MNIST onto a DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 64\n",
    "batch_size = 64\n",
    "\n",
    "dataset = datasets.MNIST(root='MNIST', transform=transforms.Compose([\n",
    "    transforms.Resize(size=image_size), # Interpolate original dataset to fit the provided size\n",
    "    transforms.ToTensor(),  # Convert the image to `numpy.ndarray`\n",
    "    transforms.Normalize(mean=mnist_mean, std=mnist_std)\n",
    "]))\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n",
    "                                         shuffle=True, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(dataset.data[0], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom weights initialization based on DCGAN paper\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_size = 100\n",
    "seed = 1\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels=noise_size, out_channels=512, kernel_size=4, stride=1, padding=0, bias=False),\n",
    "            nn.BatchNorm2d(num_features=512),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            nn.ConvTranspose2d(in_channels=512, out_channels=256, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(num_features=256),\n",
    "            nn.ReLU(True),\n",
    "            \n",
    "            nn.ConvTranspose2d(in_channels=256, out_channels=128, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(num_features=128),\n",
    "            nn.ReLU(True),\n",
    "            \n",
    "            nn.ConvTranspose2d(in_channels=128, out_channels=64, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(num_features=64),\n",
    "            nn.ReLU(True),   \n",
    "            \n",
    "            nn.ConvTranspose2d(in_channels=64, out_channels=1, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.main(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = Generator()\n",
    "generator.apply(weights_init)\n",
    "print(generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=64, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.LeakyReLU(negative_slope=0.02, inplace=True),\n",
    "\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(num_features=128),\n",
    "            nn.LeakyReLU(negative_slope=0.02, inplace=True),\n",
    "            \n",
    "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(num_features=256),\n",
    "            nn.LeakyReLU(negative_slope=0.02, inplace=True),\n",
    "            \n",
    "            nn.Conv2d(in_channels=256, out_channels=512, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(num_features=512),\n",
    "            nn.LeakyReLU(negative_slope=0.02, inplace=True),\n",
    "            \n",
    "            nn.Conv2d(in_channels=512, out_channels=1, kernel_size=4, stride=1, padding=0, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.main(x)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator = Discriminator()\n",
    "discriminator.apply(weights_init)\n",
    "print(discriminator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_label = 1.\n",
    "fake_label = 0.\n",
    "\n",
    "fixed_noise = torch.randn(size=(batch_size, noise_size, 1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.BCELoss()\n",
    "\n",
    "# Tricks to train the GANs\n",
    "optimizer_discriminator = torch.optim.SGD(params=discriminator.parameters(), lr=0.0002)\n",
    "optimizer_generator = torch.optim.Adam(params=generator.parameters(), lr=0.0002)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_discriminator = []\n",
    "loss_generator = []\n",
    "images_from_fixed_noise = []\n",
    "EPOCHS = 4\n",
    "counter = 0\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    for i, batch_data in tqdm(enumerate(dataloader, 0)):\n",
    "        # batch_data contains a pair of lists: < images `numpy.ndarray`, true labels >\n",
    "\n",
    "        # #############################################\n",
    "        # Training the Discriminator with a real batch\n",
    "        discriminator.zero_grad()  # \"PyTorch accumulates the gradients on subsequent backward passes\"\n",
    "        batch = batch_data[0].size(0)\n",
    "        labels = torch.full(size=(batch,), fill_value=np.random.normal(1., 0.1, 1)[0], dtype=torch.float)  # list of real labels\n",
    "        discriminator_from_real_output = discriminator(batch_data[0]).view(-1)  # forward propagation\n",
    "        loss_discriminator_from_real_samples = loss(discriminator_from_real_output, labels)  # calculate the loss\n",
    "        loss_discriminator_from_real_samples.backward()  # back-propagate the gradient of the loss\n",
    "        d_x = discriminator_from_real_output.mean().item()  # get the predicted ouput, should converge to 0.5\n",
    "        \n",
    "        # #############################################\n",
    "        # Training the Discriminator with a fake batch\n",
    "        fake_noise = torch.randn(size=(batch, noise_size, 1, 1))  # generate latent fake vectors\n",
    "        fake_batch = generator(fake_noise)  # generate a fake batch of images by noise\n",
    "        labels.fill_(value=np.random.normal(0.1, 0.05, 1)[0])  # list of fake labels\n",
    "        discriminator_from_fake_output = discriminator(fake_batch.detach()).view(-1)\n",
    "        loss_discriminator_from_fake_samples = loss(discriminator_from_fake_output, labels)\n",
    "        loss_discriminator_from_fake_samples.backward()  # back-propagate the gradient of the loss\n",
    "        d_g_z_1 = loss_discriminator_from_fake_samples.mean().item()  # get the predicted ouput, should converge to 0.5\n",
    "\n",
    "        loss_discriminator_from_real_and_fake_samples = loss_discriminator_from_real_samples + loss_discriminator_from_fake_samples\n",
    "        optimizer_discriminator.step()\n",
    "\n",
    "        # #############################################\n",
    "        # Training the Generator with random noise\n",
    "        generator.zero_grad()\n",
    "        labels.fill_(value=real_label)\n",
    "        generator_from_noise_output = discriminator(fake_batch).view(-1)\n",
    "        loss_generator_from_noise = loss(generator_from_noise_output, labels)\n",
    "        loss_generator_from_noise.backward()\n",
    "        d_g_z_2 = loss_generator_from_noise.mean().item()\n",
    "        optimizer_generator.step()\n",
    "\n",
    "        # Accessibility\n",
    "        loss_d = np.round(loss_discriminator_from_real_and_fake_samples.item(), 2)\n",
    "        loss_g = np.round(loss_generator_from_noise.item(), 2)\n",
    "        if i % 100 == 0:\n",
    "            print(f\"Epoch #{epoch} \\t Loss_D={loss_d} \\t Loss_G={loss_g} \\t D(x)={d_x} \\t D(G(z_1))={d_g_z_1} \\t D(G(z_2))={d_g_z_2}\")\n",
    "\n",
    "        loss_discriminator.append(loss_discriminator_from_real_and_fake_samples.item())\n",
    "        loss_generator.append(loss_generator_from_noise.item())\n",
    "\n",
    "        if (counter % 100 == 0) or ((epoch == EPOCHS-1) and (i == len(dataloader)-1)):\n",
    "            with torch.no_grad():\n",
    "                fake = generator(fixed_noise).detach().cpu()\n",
    "            images_from_fixed_noise.append(vutils.make_grid(fake, padding=2, normalize=True))\n",
    "\n",
    "        counter += 1\n",
    "    generator.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8,8))\n",
    "plt.axis(\"off\")\n",
    "ims = [[plt.imshow(np.transpose(i,(1,2,0)), animated=True)] for i in images_from_fixed_noise]\n",
    "ani = animation.ArtistAnimation(fig, ims, interval=1000, repeat_delay=1000, blit=True)\n",
    "\n",
    "HTML(ani.to_jshtml())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ani.save('dcgan_mnist.mp4', metadata={'author':'MUDSS'})"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "64689e8cfaf3b1a6014f0d7f2f290c1849c0d60709909de228356e490e4bb3ce"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
