{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ouEGhXeglgV0",
        "outputId": "c7364dba-e3ed-4c29-e814-0b2b75e3ba84"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "import os\n",
        "import torch\n",
        "from torch import nn,optim\n",
        "from torch.functional import F\n",
        "import random\n",
        "import copy\n",
        "import gym\n",
        "import numpy as np\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim.lr_scheduler import ExponentialLR\n",
        "from torch.autograd import Variable\n",
        "import pickle\n",
        "\n",
        "env = gym.make('CartPole-v1')\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "FloatTensor = torch.cuda.FloatTensor if torch.cuda.is_available() else torch.FloatTensor\n",
        "\n",
        "\n",
        "#DQN agent\n",
        "class Network(nn.Module):\n",
        "    def __init__(self,action_size,state_size):\n",
        "        super(Network, self).__init__()\n",
        "        self.hidden=nn.Linear(state_size,64)\n",
        "        self.hidden2=nn.Linear(64,128)\n",
        "        self.out=nn.Linear(128,action_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x=x.to(device)\n",
        "        x=F.relu(self.hidden(x))\n",
        "        x=F.relu(self.hidden2(x))\n",
        "        x=self.out(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class DQN():\n",
        "    def __init__(self,gamma,action_shape,state_shape):\n",
        "        self.memory=[]\n",
        "        self.model=Network(action_shape,state_shape).to(device)\n",
        "        self.target_model=copy.deepcopy(self.model)\n",
        "        self.gamma=gamma\n",
        "        self.done_mem=[]\n",
        "        self.change=0\n",
        "        self.num_action=action_shape\n",
        "        self.optimizer=optim.Adam(self.model.parameters(),lr=0.00025)\n",
        "        self.scheduler = ExponentialLR(self.optimizer, gamma=0.99)\n",
        "    def reload(self,location):\n",
        "      state_dict = torch.load(location)\n",
        "      self.model.load_state_dict(state_dict)\n",
        "      input=open(\"memory.dat\",'rb')\n",
        "      self.memory=pickle.load(input)\n",
        "      self.target_model=copy.deepcopy(self.model)\n",
        "    def save(self):\n",
        "      torch.save(self.target_model.state_dict(), 'checkpoint_.pth')\n",
        "      out= open(\"memory.dat\",'wb')\n",
        "      pickle.dump(self.memory,out)\n",
        "      out.close()\n",
        "    def exploit(self,state):\n",
        "        with torch.no_grad():\n",
        "            return torch.argmax(self.predict(self.target_model,state))\n",
        "    def add_done(self,episode):\n",
        "      self.done_mem.append(episode)\n",
        "    def add_episode(self,episode):\n",
        "        if len(self.memory)>50000:\n",
        "            self.memory.pop(random.randrange(len(self.memory)))\n",
        "        self.memory.append(episode)\n",
        "    def predict(self,model,state):\n",
        "        return model(torch.from_numpy(state).float())\n",
        "    def act(self,state,epsilon,env):\n",
        "        p=random.uniform(0,1)\n",
        "        if p<epsilon:\n",
        "            return env.action_space.sample()\n",
        "        else:\n",
        "            with torch.no_grad():\n",
        "                return torch.argmax(self.predict(self.model,state))\n",
        "    def train(self,batch_size):\n",
        "        \n",
        "        self.optimizer.zero_grad()\n",
        "        transitions = random.sample(self.memory,batch_size)\n",
        "        batch_state,batch_action,batch_reward,batch_next_state = zip(*transitions)\n",
        "        batch_state,batch_action,batch_reward,batch_next_state=Variable(torch.cat(batch_state)), Variable(torch.cat(batch_action)), Variable(torch.cat(batch_reward)),Variable(torch.cat(batch_next_state))\n",
        "        prediction=self.model(batch_state).gather(1, batch_action.type(torch.int64).unsqueeze(1)).squeeze(1)\n",
        "        with torch.no_grad():\n",
        "            max_q=self.target_model(batch_next_state).detach().max(1)[0]\n",
        "            label=batch_reward+self.gamma*max_q\n",
        "        loss=nn.SmoothL1Loss()(prediction,label)\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "        self.target_model=copy.deepcopy(self.model)\n",
        "        self.train_done(batch_size)\n",
        "\n",
        "        \n",
        "    def train_done(self,batch_size):\n",
        "        self.optimizer.zero_grad()\n",
        "        transitions = random.sample(self.done_mem,batch_size)\n",
        "        batch_state, batch_action,batch_reward= zip(*transitions)\n",
        "        batch_state, batch_action,batch_reward=Variable(torch.cat(batch_state)), Variable(torch.cat(batch_action)), Variable(torch.cat(batch_reward))\n",
        "        prediction=self.model(batch_state).gather(1, batch_action.type(torch.int64).unsqueeze(1)).squeeze(1)\n",
        "        loss=nn.SmoothL1Loss()(prediction,batch_reward)\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "    def change_target(self):\n",
        "        self.target_model=copy.deepcopy(self.model)\n",
        "        self.change+=1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ByDELqZeQLda"
      },
      "outputs": [],
      "source": [
        "agent=DQN(0.8,env.action_space.n,env.observation_space.shape[0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kfAY9gOlQNM5"
      },
      "outputs": [],
      "source": [
        "agent.reload('checkpoint_.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for i in range(5000):\n",
        "\ttotal_training_rewards = 0\n",
        "\tobservation = env.reset()\n",
        "\tdone = False\n",
        "\twhile not done:\n",
        "\t\t\t#env.render()\n",
        "\t\t\taction=env.action_space.sample()\n",
        "\t\t\tnew_observation, reward, done, info = env.step(action)\n",
        "\t\t\tif not done:\n",
        "\t\t\t\tagent.add_episode((FloatTensor([observation]),FloatTensor([action]),FloatTensor([reward]),FloatTensor([new_observation])))\n",
        "\t\t\telse:\n",
        "\t\t\t\tagent.add_done((FloatTensor([observation]),FloatTensor([action]),FloatTensor([reward])))\n",
        "\t\t\tobservation = new_observation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mzuR9mGH8UNB",
        "outputId": "dbec0661-30c3-489b-8f61-9b35dd9b5300"
      },
      "outputs": [],
      "source": [
        "\n",
        "epsilon=0.2\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "time=0\n",
        "for i in range(200):\n",
        "\t\tepsilon*=0.99\n",
        "\t\ttotal_training_rewards = 0\n",
        "\t\tobservation = env.reset()\n",
        "\t\tdone = False\n",
        "\t\t\n",
        "\t\twhile not done:\n",
        "\t\t\t\t#env.render()\n",
        "\t\t\t\ttime+=1\n",
        "\t\t\t\taction=agent.act(observation,epsilon,env)\n",
        "\t\t\t\tnew_observation, reward, done, info = env.step(int(action))\n",
        "\t\t\t\ttotal_training_rewards+=reward\n",
        "\t\t\t\tif not done:\n",
        "\t\t\t\t\tagent.add_episode((FloatTensor([observation]),FloatTensor([action]),FloatTensor([reward]),FloatTensor([new_observation])))\n",
        "\t\t\t\telse:\n",
        "\t\t\t\t\tif total_training_rewards<500:\n",
        "\t\t\t\t\t\tagent.add_done((FloatTensor([observation]),FloatTensor([action]),FloatTensor([reward])))\n",
        "\t\t\t\tobservation = new_observation\n",
        "\t\t\t\tif time%4==0:\n",
        "\t\t\t\t\tagent.train(50)\n",
        "\t\t\t\tif time%100==0:\n",
        "\t\t\t\t\tagent.change_target()\n",
        "\t\tprint(total_training_rewards)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DMJH-IvZxsF7"
      },
      "outputs": [],
      "source": [
        "agent.save()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for i in range(5):\n",
        "    total_training_rewards = 0\n",
        "    observation = env.reset()\n",
        "    done = False\n",
        "    while not done:\n",
        "        env.render()\n",
        "        action=agent.exploit(observation)\n",
        "        new_observation, reward, done, info = env.step(int(action))\n",
        "        observation = new_observation\n",
        "        total_training_rewards+=reward\n",
        "    print(total_training_rewards)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Untitled1.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
